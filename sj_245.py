# -*- coding: utf-8 -*-
"""SJ-245.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11xyZAdzOvUTxv0K-SZMeEpxIaA1j1Bi0

#Installing PySpark & Hadoop
"""

# Installing Java (required for Spark)
!apt-get install openjdk-8-jdk-headless -qq > /dev/null

# Setting Java environment variables
import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"

# Installing PySpark
!pip install pyspark
!pip install findspark

# Initializing Spark
import findspark
findspark.init()

"""# importing all necessary modules"""

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, count, when, isnan
from pyspark.sql.functions import median
from pyspark.sql.functions import mean, stddev
import matplotlib.pyplot as plt
import seaborn as sns
from pyspark.ml.regression import LinearRegression
from pyspark.ml.evaluation import RegressionEvaluator
from pyspark.ml.feature import VectorAssembler, StringIndexer
from pyspark.ml.regression import GBTRegressor
from pyspark.ml.evaluation import RegressionEvaluator
from pyspark.ml import Pipeline
from pyspark.ml.classification import RandomForestClassifier
from pyspark.ml.evaluation import MulticlassClassificationEvaluator
from pyspark.mllib.evaluation import MulticlassMetrics
from pyspark.ml.classification import LogisticRegression
from pyspark.ml.evaluation import MulticlassClassificationEvaluator
from pyspark.ml.classification import OneVsRest
import pandas as pd

import warnings
warnings.filterwarnings("ignore")

"""#Loading Data &  Cleaning"""

# Initializing Spark session
spark = SparkSession.builder \
    .appName("UsedCarAnalysis") \
    .getOrCreate()

# Loading dataset
data = spark.read.csv("/content/car_prices.csv", header=True, inferSchema=True)

# Showing schema & data
data.printSchema()

data.show(5)

# Checking missing values
data.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in data.columns]).show()

# Dropping rows where critical columns are missing
critical_cols = ["sellingprice", "year", "make", "model", "odometer"]
data_clean = data.dropna(subset=critical_cols)

data_clean = data_clean.fillna({
    "trim": "Unknown",
    "transmission": "Unknown",
    "color": "Unknown",
    "interior": "Unknown",
    "condition": 3
})

# Dropping the 4 rows with missing VIN
data_final = data_clean.dropna(subset=["vin"])

mmr_median = data_final.select(median("mmr")).collect()[0][0]
data_final = data_final.fillna({"mmr": mmr_median})

# Verify cleaning
print(f"Rows after cleaning: {data_clean.count()}")

# Calculating mean ± 3σ for odometer
stats = data_clean.select(
    mean("odometer").alias("mean"),
    stddev("odometer").alias("stddev")
).collect()

mean_odo = stats[0]["mean"]
stddev_odo = stats[0]["stddev"]

# Filter outliers (3 standard deviations)
data_clean = data_clean.filter(
    (col("odometer") > (mean_odo - 3*stddev_odo)) &
    (col("odometer") < (mean_odo + 3*stddev_odo)))

# Drop the 4 rows with missing VIN after outlier filtering
data_clean = data_clean.dropna(subset=["vin"])

# Fill nulls in mmr after outlier filtering
mmr_median = data_clean.select(median("mmr")).collect()[0][0]
data_clean = data_clean.fillna({"mmr": mmr_median})

"""# Exploratory Data Analysis"""

year_price = data_clean.groupBy('year').agg(
    mean('sellingprice').alias('avg_price'),
    count('*').alias('count')
).orderBy('year').toPandas()

plt.figure(figsize=(12,6))
sns.lineplot(x='year', y='avg_price', data=year_price)
plt.fill_between(year_price['year'],
                year_price['avg_price']*0.95,
                year_price['avg_price']*1.05, alpha=0.2)
plt.title('Average Price by Model Year')
plt.xlabel('Year')
plt.ylabel('Average Price ($)')
plt.show()

brand_price = data_clean.groupBy('make').agg(
    mean('sellingprice').alias('avg_price'),
    count('*').alias('count')
).orderBy('avg_price', ascending=False).limit(10).toPandas()

plt.figure(figsize=(12,6))
sns.barplot(x='avg_price', y='make', data=brand_price, palette='rocket')
plt.title('Top 10 Brands by Average Price')
plt.xlabel('Average Price ($)')
plt.ylabel('Brand')
plt.show()

condition_analysis = data_clean.groupBy('condition').agg(
    mean('sellingprice').alias('avg_price'),
    mean('odometer').alias('avg_mileage'),
    count('*').alias('count')
).orderBy('condition').toPandas()

fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16,6))

sns.barplot(x='condition', y='avg_price', data=condition_analysis, ax=ax1)
ax1.set_title('Average Price by Condition Rating')
ax1.set_xlabel('Condition (1-5)')
ax1.set_ylabel('Average Price ($)')

sns.barplot(x='condition', y='avg_mileage', data=condition_analysis, ax=ax2)
ax2.set_title('Average Mileage by Condition Rating')
ax2.set_xlabel('Condition (1-5)')
ax2.set_ylabel('Average Mileage (miles)')

plt.tight_layout()
plt.show()

transmission_df = data_clean.groupBy("transmission").agg(
    mean("sellingprice").alias("avg_price"),
    count("*").alias("count")
).orderBy("avg_price", ascending=False).toPandas()

plt.figure(figsize=(10,5))
sns.barplot(x="transmission", y="avg_price", data=transmission_df, palette="Blues_d")
plt.title("Average Price by Transmission Type")
plt.xlabel("")
plt.ylabel("Average Price ($)")
plt.show()

from pyspark.sql.functions import col, mean, count

# top colors by average price
color_df = data_clean.groupBy("color").agg(
    mean("sellingprice").alias("avg_price"),
    count("*").alias("count")
).filter(col("count") > 1000).orderBy("avg_price", ascending=False).limit(10).toPandas()

# Creating a color mapping dictionars
color_palette = {
    "white": "#FFFFFF",
    "black": "#000000",
    "silver": "#C0C0C0",
    "gray": "#808080",
    "red": "#FF0000",
    "blue": "#0000FF",
    "green": "#00FF00",
    "yellow": "#FFFF00",
    "orange": "#FFA500",
    "brown": "#A52A2A"
}

# Mapping colors to their hex values
colors = [color_palette.get(c.lower(), "#CCCCCC") for c in color_df["color"]]

plt.figure(figsize=(12,6))
sns.barplot(x="color", y="avg_price", data=color_df, palette=colors)
plt.title("Top 10 Colors by Average Price (Min 1,000 Listings)")
plt.xlabel("")
plt.ylabel("Average Price ($)")
plt.show()

"""# Feature Engineering"""

indexers = [StringIndexer(inputCol=c, outputCol=f"{c}_index", handleInvalid="keep")
            for c in ["make", "model", "transmission", "color"]]

assembler = VectorAssembler(
    inputCols=["year", "odometer", "mmr", "make_index", "model_index"],
    outputCol="features"
)

# Pipeline
pipeline = Pipeline(stages=indexers + [assembler])
data_prepped = pipeline.fit(data_clean).transform(data_clean)

# Train-Test Split
train_data, test_data = data_prepped.randomSplit([0.8, 0.2], seed=42)

"""# Model Training

## Linear Regression
"""

lr = LinearRegression(featuresCol="features", labelCol="sellingprice")
lr_model = lr.fit(train_data)

# Predictions
lr_predictions = lr_model.transform(test_data)

# Evaluation
evaluator = RegressionEvaluator(labelCol="sellingprice", predictionCol="prediction")
print(f"RMSE: {evaluator.evaluate(lr_predictions, {evaluator.metricName: 'rmse'})}")
print(f"R²: {evaluator.evaluate(lr_predictions, {evaluator.metricName: 'r2'})}")

# Residual Plot
residuals = lr_predictions.select("prediction", "sellingprice").toPandas()
plt.figure(figsize=(10,6))
sns.residplot(x="prediction", y="sellingprice", data=residuals, lowess=True)
plt.title("Residual Plot")
plt.xlabel("Predicted Values")
plt.ylabel("Residuals")
plt.show()

"""## Gradient Boosted Trees (GBT)"""

# Model Training
gbt = GBTRegressor(featuresCol="features", labelCol="sellingprice", maxIter=50, maxBins=1024) # Increased maxBins
gbt_model = gbt.fit(train_data)

# Predictions
gbt_predictions = gbt_model.transform(test_data)

# Evaluation
evaluator = RegressionEvaluator(labelCol="sellingprice", predictionCol="prediction")
print(f"RMSE: {evaluator.evaluate(gbt_predictions, {evaluator.metricName: 'rmse'})}")
print(f"R²: {evaluator.evaluate(gbt_predictions, {evaluator.metricName: 'r2'})}")